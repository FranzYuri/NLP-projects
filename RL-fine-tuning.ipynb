{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcayLcTltUeT"
      },
      "source": [
        "<center>\n",
        "<h1>\n",
        "<h1>APM 53674: ALTeGraD</h1>\n",
        "<h2>Lab Session 3: Improving LLMs with RLHF (DPO & GRPO)</h2>\n",
        "<h4>Lecture: Prof. Michalis Vazirgiannis<br>\n",
        "Lab: Yang Zhang and Xiao Fei</h4>\n",
        "<h5>Tuesday, October 14, 2025</h5>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "<hr style=\"border:10px solid gray\"> </hr>\n",
        "<p style=\"text-align: justify;\">\n",
        "This handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>October 19\n",
        ", 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late ‚Üí -5 pts; ]24, 48] hours late ‚Üí -10 pts; > 48 hours late ‚Üí not graded (zero).\n",
        "</p>\n",
        "<hr style=\"border:5px solid gray\"> </hr>\n",
        "\n",
        "\n",
        "# ü§ñ Post-Training: Improving LLMs with RLHF (DPO & GRPO)\n",
        "\n",
        "In this notebook, we‚Äôll show how to improve a language model using **two post-training techniques**:\n",
        "\n",
        "### üß† What You‚Äôll Learn\n",
        "- What **Direct Preference Optimization (DPO)** is and how it helps models choose better answers.\n",
        "- What **Group Relative Policy Optimization (GRPO)** is and how it helps models solve tasks  improving reasoning and performance on complex tasks (math, code, logic).\n",
        "- How to train small models on **real feedback data**.\n",
        "- How to **observe changes in model behavior** after fine-tuning.\n",
        "\n",
        "\n",
        "### üì¶ What We‚Äôll Use\n",
        "- **Hugging Face ü§ó Transformers** to load and run models\n",
        "- **TRL (Transformer Reinforcement Learning)** library by Hugging Face ü§ó for DPO and GRPO\n",
        "- **A small version** of the French translated [Anthropic HH-RLHF dataset](https://huggingface.co/datasets/AIffl/french_hh_rlhf)\n",
        "- **Colab GPU**, so models are small enough to run quickly\n",
        "\n",
        "### Useful links:\n",
        "- [Hugging Face ü§ó DPO Trainer](https://huggingface.co/docs/trl/dpo_trainer)\n",
        "- [Hugging Face ü§ó GRPO Trainer](https://huggingface.co/docs/trl/grpo_trainer)\n",
        "\n",
        "> This notebook is interactive, friendly, and high-level. You don‚Äôt need to know deep math or theory to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wH0u4hhtUeV"
      },
      "source": [
        "# Quick start:\n",
        "1-  Clone the repository:\n",
        "```bash\n",
        "git clone https://github.com/BounharAbdelaziz/RLHF.git\n",
        "```\n",
        "2- Install the dependencies:\n",
        "```bash\n",
        "pip install -q -r requirements.txt\n",
        "```\n",
        "Now we are ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7EtCgK2t1lf"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPEWV5M_tUeV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/BounharAbdelaziz/RLHF.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAXzTCmJtX3s"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r RLHF/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A8MAR6WtUeV"
      },
      "source": [
        "# Part I: DPO\n",
        "\n",
        "# üß† Fine-tuning Qwen2.5-0.5B-Instruct on French Data\n",
        "\n",
        "In this part, we‚Äôll walk through how to **fine-tune the Qwen2.5-0.5B-Instruct** model on **French-language data**, using **off-policy DPO (Direct Preference Optimization)**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Key Concepts\n",
        "\n",
        "- **Model**: [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)  \n",
        "- **Objective**: Adapt the model for French understanding and instruction-following  \n",
        "- **Method**: Off-policy **DPO** for alignment-based fine-tuning  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è System Requirements\n",
        "\n",
        "Before training, make sure you choose an appropriate GPU setup.\n",
        "\n",
        "- **Memory requirements**: [GPU memory guidance](https://rahulschand.github.io/gpu_poor/)  \n",
        "- **GPU options**:\n",
        "  - AWS SageMaker: [pricing calculator](https://aws.amazon.com/sagemaker-ai/pricing/)  \n",
        "  - üí∏ Cheaper alternative: [RunPod](https://www.runpod.io/)  \n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Memory Optimization\n",
        "\n",
        "We‚Äôll use **LoRA** (Low-Rank Adaptation) combined with **quantization (4-bit)** to reduce GPU memory usage while maintaining performance.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Evaluation Tools\n",
        "\n",
        "To evaluate the fine-tuned model, you can use one or more of the following frameworks:\n",
        "\n",
        "- [`lm-eval-harness`](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "- [`lighteval`](https://github.com/huggingface/evaluate)\n",
        "- [`libra-eval`](https://github.com/facebookresearch/libra)\n",
        "\n",
        "---\n",
        "Below is a visual overview of the **Direct Preference Optimization (DPO)** training process:\n",
        "\n",
        "![DPO Training Diagram](https://1drv.ms/i/c/ae69638675180117/IQQ_wS77RKdrS4tzLbwoqr1gAR0Bf_1X2U36NRBp1Odsypg?width=560&height=48)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AyeUIktUeV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from trl import (\n",
        "    DPOTrainer,\n",
        "    DPOConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "import torch\n",
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czRsmsAQEF4"
      },
      "source": [
        "## üìä Tracking with Weights & Biases (W&B)\n",
        "\n",
        "We‚Äôll use **Weights & Biases (W&B)** to log training metrics, model versions, and system stats so you can compare runs, debug faster, and share results. Before running the next cell, **create a free account** at [https://wandb.ai](https://wandb.ai) and make a new **Project** (e.g., `RLHF`). In the code cell that follows, we‚Äôll initialize W&B; on first use you‚Äôll need to **register** then paste your **API key** from your W&B profile. During training, W&B will automatically track losses, learning rate, gradient norms, and GPU utilization, and we‚Äôll log custom metrics (e.g., validation perplexity, evaluation scores) plus configuration details (dataset, hyperparameters, LoRA/quantization settings). Each run will appear on your project dashboard with charts, tables, run metadata, and artifacts, making it easy to **compare experiments**, **resume runs**, and **share dashboards** with your teammates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vtHPrUXYYVO"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "Create your Weights&Biases account and fill the gap the next cell.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6J6EMxdtUeV"
      },
      "outputs": [],
      "source": [
        "WANDB_API_KEY = \"eed8a3b97053630ff58d2a890c31fd523b79fe8d\" \n",
        "\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "os.environ[\"WANDB_PROJECT\"] = \"RLHF\"\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "DATASET_PATH = \"AIffl/french_orca_dpo_pairs\" \n",
        "\n",
        "\n",
        "LIMIT = 2_000\n",
        "\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "SEED = 1998\n",
        "\n",
        "MAX_PROMPT_LEN = 1024\n",
        "MAX_LENGTH = MAX_PROMPT_LEN + 512\n",
        "\n",
        "RUN_NAME = \"DPO-french-orca-\" + MODEL_NAME.split('/')[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS_Fgnc4tUeW"
      },
      "source": [
        "## Load the SFT Model and Tokenizer\n",
        "\n",
        "We‚Äôll stick with 4-bit quantization via bitsandbytes for this lab. You‚Äôve already used it last week, so nothing new‚Äîsame setup (load the model with 4-bit weights), same goal (reduce VRAM) with minimal impact on quality for our use case. This keeps runs feasible on a single GPU.\n",
        "\n",
        "We will need the tonkenizer in the data preparation step to apply the chat template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzt-X_0mYnEl"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Create the quantization confiduration to load the model with 4 bits\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6vQ5ha430Ke"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit=True,\n",
        "              bnb_4bit_quant_type=\"nf4\",\n",
        "              bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx0a87p1tUeW"
      },
      "outputs": [],
      "source": [
        "# Quantization configuration\n",
        "quantization_config = bnb_config\n",
        "\n",
        "# Load the model to finetune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "# to avoid warning\n",
        "model.config.use_cache = False\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wb1mjaltUeW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "### üí¨ Chat templates & converting data to `messages`\n",
        "\n",
        "Modern instruction-tuned models (including **Qwen2.5-0.5B-Instruct**) expect inputs in a **chat format** and rely on a **tokenizer chat template** to turn structured messages into the exact token sequence the model was trained on. In practice, you should **not** hand-craft special tokens; instead, pass a list of `{role, content}` messages to the tokenizer and let `apply_chat_template(...)` do the right thing.\n",
        "\n",
        "#### Why a chat template?\n",
        "- Ensures your prompts match the **pretraining/finetuning format** (system/user/assistant turns, BOS/EOS, separators).\n",
        "- Minimizes prompt drift across libraries and models.\n",
        "- Makes it easy to add **system instructions** (e.g., ‚ÄúYou are a helpful assistant that answers in French.‚Äù).\n",
        "\n",
        "#### Message structure\n",
        "Each example becomes an ordered list of chat turns:\n",
        "```python\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"Tu es un assistant utile. R√©ponds en fran√ßais.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Explique la diff√©rence entre LoRA et le fine-tuning complet.\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"LoRA adapte un petit sous-espace de poids, alors que...\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3h1j7ezZL4R"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "Create the user message which is the question field of the dataset.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Wf77GCaYyP"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_PATH, split=f\"train\")\n",
        "dataset.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW5vfRm0tUeW"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_dpo(example):\n",
        "    # Format system message if present\n",
        "    messages = []\n",
        "    if example.get('system') and len(example['system'].strip()) > 0:\n",
        "        messages.append({\"role\": \"system\", \"content\": example['system']})\n",
        "\n",
        "    user_message = {\"role\": \"user\", \"content\": example[\"question\"]}\n",
        "    messages.append(user_message)\n",
        "\n",
        "    # Create prompt with generation prompt for DPO\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # The chosen and rejected should be the assistant responses only\n",
        "    chosen = example['chosen']\n",
        "    rejected = example['rejected']\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }\n",
        "\n",
        "# Download the training dataset\n",
        "dataset = load_dataset(DATASET_PATH, split=f\"train\")\n",
        "# shuffle and select a number of samples\n",
        "dataset = dataset.shuffle(True).select(range(LIMIT))\n",
        "\n",
        "# Save columns\n",
        "original_columns = dataset.column_names\n",
        "\n",
        "# Apply the preprocessing function\n",
        "dpo_dataset = dataset.map(\n",
        "    preprocess_for_dpo,\n",
        "    remove_columns=original_columns,\n",
        ")\n",
        "\n",
        "# Filter out examples that are too long\n",
        "def filter_length(example):\n",
        "    prompt_length = len(tokenizer.encode(example['prompt']))\n",
        "    chosen_length = len(tokenizer.encode(example['chosen']))\n",
        "    rejected_length = len(tokenizer.encode(example['rejected']))\n",
        "\n",
        "    return (prompt_length + max(chosen_length, rejected_length)) < MAX_LENGTH\n",
        "\n",
        "dpo_dataset = dpo_dataset.filter(filter_length)\n",
        "\n",
        "print(f\"Dataset size after filtering: {len(dpo_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nrGIGktUeW"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Training will mirror last week‚Äôs lab, but we‚Äôll switch from SFT to **off-policy DPO** using the `trl` library. Concretely, we‚Äôll instantiate a **policy model** (trainable) and a **reference model** (frozen) and optimize with the DPO objective so the policy prefers **chosen** over **rejected** responses for the same prompt.\n",
        "\n",
        "### What we‚Äôll use\n",
        "- **TRL**: `DPOConfig`, `DPOTrainer`\n",
        "- **PEFT**: LoRA adapters on top of the base **Qwen2.5-0.5B-Instruct**\n",
        "- **Quantization**: 4-bit (QLoRA-style) to fit on small GPUs\n",
        "- **Logging**: W&B for metrics, configs, and artifacts\n",
        "\n",
        "### Expected dataset columns\n",
        "- `prompt` (or `messages`): the shared context (system+user turns)\n",
        "- `chosen`: assistant reply preferred by annotators\n",
        "- `rejected`: less-preferred reply\n",
        "> If you‚Äôre keeping everything in chat format, we‚Äôll pass lists of `{role, content}` and rely on `tokenizer.apply_chat_template(...)` inside the collator.\n",
        "\n",
        "### Minimal training flow\n",
        "1. Load tokenizer with the **chat template** and enable 4-bit loading of the base model.\n",
        "2. Wrap the model with **LoRA** (target attention/MLP modules).\n",
        "3. Build a `datasets.Dataset` that yields `(prompt/messages, chosen, rejected)`.\n",
        "4. Define `DPOConfig` (batch size, lr, epochs, `beta`, logging/saving/eval cadence).\n",
        "5. Create `DPOTrainer(policy_model, ref_model, tokenizer, train_dataset, eval_dataset, **config)`.\n",
        "6. Call `trainer.train()`; optional `trainer.evaluate()` and `trainer.save_model()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNKXz-pbaj0l"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Create the lora config with rank of 32, alpha of 64 and dropout of 0.1 on all MLP layers (execluding Embedding layers) and train the model\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJeBW-l6tUeW"
      },
      "outputs": [],
      "source": [
        "# LoRA configuration - targeting the correct modules for Qwen2.5\n",
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "          \"q_proj\",\n",
        "          \"k_proj\",\n",
        "          \"v_proj\",\n",
        "          \"o_proj\",\n",
        "          \"gate_proj\",\n",
        "          \"up_proj\",\n",
        "          \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training configuration\n",
        "training_args = DPOConfig(\n",
        "    beta=0.1,  # DPO temperature parameter\n",
        "    learning_rate=5e-6,\n",
        "    max_prompt_length=MAX_PROMPT_LEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    per_device_train_batch_size=1,  # Reduced for memory\n",
        "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size of 4 (1*4)\n",
        "    num_train_epochs=1,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=1,\n",
        "    save_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",  # More memory efficient\n",
        "    warmup_ratio=0.03, # 3% of the steps will be just a warmup\n",
        "    save_strategy=\"steps\",\n",
        "    output_dir=\"./dpo_model\",\n",
        "    report_to=\"wandb\",\n",
        "    run_name=RUN_NAME,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "# Initialize the trainer - Note: no ref_model needed when using peft_config\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,  # This automatically handles reference model\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dpo_dataset,\n",
        ")\n",
        "\n",
        "print(\"Sample from dataset:\")\n",
        "print(f\"Prompt: {dpo_dataset[0]['prompt']}\")\n",
        "print(f\"Chosen: {dpo_dataset[0]['chosen']}\")\n",
        "print(f\"Rejected: {dpo_dataset[0]['rejected']}\")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm3zJDWVtUeW"
      },
      "outputs": [],
      "source": [
        "# merge LoRA adapters with the base model\n",
        "save_path = \"dpo_model/final_merged_dpo_model\"\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkbrN3GltUeW"
      },
      "source": [
        "## Model Testing\n",
        "\n",
        "We will test the DPO model via chat-app\n",
        "To validate alignment gains, we‚Äôll spin up a small **Gradio** app that queries both the **pre-DPO** model (baseline/reference) and the **post-DPO** **policy** side-by-side. The UI lets you enter a French prompt, then compares generations using the **same chat template** and decoding settings. This helps spot qualitative shifts in helpfulness, safety, and instruction-following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKtCZEoxtUeW"
      },
      "outputs": [],
      "source": [
        "from RLHF.chat_app import launch_chat_app\n",
        "\n",
        "launch_chat_app(\n",
        "    model_path=\"habdine/CSC_53432_lab3_dpo\",\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (DPO vs Base) for French\",\n",
        "    DPO_TEST=True,\n",
        "    FRENCH_TEST=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvmpsUrOtUeX"
      },
      "outputs": [],
      "source": [
        "launch_chat_app(\n",
        "    model_path=\"BounharAbdelaziz/Qwen2.5-0.5B-DPO-English-Orca\",\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (DPO vs Base) for English\",\n",
        "    DPO_TEST=True,\n",
        "    FRENCH_TEST=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb2KC8TJtUeX"
      },
      "source": [
        "# Part II: GRPO\n",
        "\n",
        "\n",
        "The following diagram illustrates the **GRPO (Generative Reinforcement Preference Optimization)** process ‚Äî an alternative to DPO that directly optimizes generation quality from preference data using reinforcement-style updates:\n",
        "\n",
        "![GRPO Training Overview](https://1drv.ms/i/c/ae69638675180117/IQQ-KizPdUxCRZU9qDGcpX1AAeettH1uhsJqqM1WjXiYR6s?width=705&height=66)\n",
        "\n",
        "After exploring DPO, we now move on to **GRPO** ‚Äî a reinforcement learning‚Äìstyle approach that builds directly on preference data.  \n",
        "While **DPO** adjusts the model using an *analytic loss* derived from preference pairs, **GRPO** takes a more dynamic route: it uses **reward modeling and policy gradients** to optimize the model through sampled generations.\n",
        "\n",
        "In essence:\n",
        "- GRPO **learns from human (or model) preferences** using *on-policy* updates.  \n",
        "- It combines elements of **PPO** (Proximal Policy Optimization) with **preference-based rewards** rather than explicit numerical scores.  \n",
        "- This allows the model to better capture *generation quality* aspects that aren‚Äôt directly expressible through static loss terms.\n",
        "\n",
        "In the next section, we‚Äôll explore how to configure and launch a **GRPO training loop** using `trl`, reusing much of our previous setup (tokenizer, LoRA, quantization) but switching to **on-policy optimization**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9K2jcjn12YY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from trl import (\n",
        "    GRPOConfig,\n",
        "    GRPOTrainer,\n",
        ")\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "WANDB_API_KEY = \"eed8a3b97053630ff58d2a890c31fd523b79fe8d\"\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "os.environ[\"WANDB_PROJECT\"] = \"RLHF\"\n",
        "wandb.login()\n",
        "\n",
        "DATASET_PATH = \"openai/gsm8k\"\n",
        "\n",
        "LIMIT = 200\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "SEED = 1998\n",
        "\n",
        "USE_LORA = True\n",
        "\n",
        "if MODEL_NAME == \"Qwen/Qwen2.5-0.5B-Instruct\":\n",
        "  USE_QUANT = False\n",
        "else:\n",
        "  USE_QUANT = True\n",
        "\n",
        "lora_alpha = 128\n",
        "lora_r = 64\n",
        "lora_dropout = 0.1\n",
        "\n",
        "if not USE_LORA:\n",
        "  MAX_PROMPT_LEN = 512\n",
        "  MAX_LENGTH = MAX_PROMPT_LEN + 512\n",
        "else:\n",
        "    MAX_PROMPT_LEN = 150\n",
        "    MAX_LENGTH = MAX_PROMPT_LEN + 150\n",
        "\n",
        "RUN_NAME = \"GRPO-GSM8K-limit-\" + str(LIMIT) + \"-\" + MODEL_NAME.split('/')[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fRTvtVGtUeX"
      },
      "source": [
        "## Load the SFT Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UkT_7Y_tUeX"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the model to finetune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quantization_config if USE_QUANT else None,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if USE_QUANT:\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if USE_LORA:\n",
        "  lora_config = LoraConfig(\n",
        "      r=lora_r,  # Rank of adaptation\n",
        "      lora_alpha=lora_alpha,  # LoRA scaling parameter\n",
        "      target_modules=[\n",
        "          \"q_proj\",\n",
        "          \"k_proj\",\n",
        "          \"v_proj\",\n",
        "          \"o_proj\",\n",
        "          \"gate_proj\",\n",
        "          \"up_proj\",\n",
        "          \"down_proj\",\n",
        "      ],  # Target modules for Qwen2.5 architecture\n",
        "      lora_dropout=lora_dropout,  # LoRA dropout\n",
        "      bias=\"none\",  # Bias type\n",
        "      task_type=TaskType.CAUSAL_LM,  # Task type\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jo1slG9tUeX"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "For GRPO training, we‚Äôll use the **GSM8K** dataset ‚Äî a benchmark of grade-school math word problems.  \n",
        "Each problem includes a **question** and a **final answer**. Our goal is to teach the model to reason in French (or English if you prefer) and **output only the final numeric answer** enclosed between `<answer>` and `</answer>` tags.\n",
        "\n",
        "This format makes automatic evaluation trivial ‚Äî we can extract the number between tags and compare it directly to the reference.\n",
        "\n",
        "With GRPO the model will learn two main things:\n",
        "- How follow the instruction to output the correct format.\n",
        "- Gain more math capabilities\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why structure as chat messages?\n",
        "\n",
        "Just like with DPO, the **Qwen2.5-0.5B-Instruct** model expects inputs in a *chat-style message format*.  \n",
        "We‚Äôll use:\n",
        "- A **system message** to define the task and output style.\n",
        "- A **user message** with the math problem.\n",
        "- An **assistant message** containing the reasoning and final numeric answer wrapped in tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQ-3KLwb242"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Create your training samples as following:<br>\n",
        "1- system message: R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS <br>\n",
        "2- one-shot example of user message of \"What is 2+2?\" and an assistant message of \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n$<answer>$4$</answer>$\" <br>\n",
        "3- the question sample from the dataset as a user message.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa0rL_52hBtH"
      },
      "outputs": [],
      "source": [
        "if LIMIT:\n",
        "    dataset = load_dataset(DATASET_PATH, \"main\", split=f\"train[:{LIMIT}]\")  # Small subset for demo\n",
        "else:\n",
        "    dataset = load_dataset(DATASET_PATH, \"main\")\n",
        "\n",
        "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
        "The assistant first thinks shortly about the reasoning process in the mind and then provides the user with the answer in a new line between <answer> and </answer>.\"\"\"\n",
        "\n",
        "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\"\n",
        "\n",
        "dataset.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U95rr5j-tUeX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_dataset(dataset, chunk_size=1000) -> Dataset:\n",
        "\n",
        "    def extract_hash_answer(text: str) -> str | None:\n",
        "        try:\n",
        "            return text.split(\"####\")[1].strip()\n",
        "        except IndexError:\n",
        "            return None\n",
        "\n",
        "    def process_batch(batch):\n",
        "        prompts = [\n",
        "            [\n",
        "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
        "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
        "                {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n<answer>4</answer>\"},\n",
        "                {'role': 'user', 'content': q.strip()}\n",
        "            ] for q in batch['question']\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'prompt': prompts,\n",
        "            'answer': [extract_hash_answer(a) for a in batch['answer']]\n",
        "        }\n",
        "\n",
        "    return dataset.map(process_batch, batched=True, batch_size=chunk_size)\n",
        "train_dataset = preprocess_dataset(dataset, chunk_size=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra_X_fK-22Ve"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvwShnXPhJxI"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AsDSYi3tUeX"
      },
      "source": [
        "## Reward Function Design\n",
        "\n",
        "We‚Äôll use **two simple rewards** during GRPO rollouts:\n",
        "\n",
        "1. **Format reward** ‚Äî checks that the **last non-empty line** is exactly in the form  \n",
        "   `<answer>NUMBER</answer>`  \n",
        "   - Score: **1** if correct format, **0** otherwise.\n",
        "\n",
        "2. **Correctness reward** ‚Äî checks whether the extracted number matches the gold answer.  \n",
        "   - Score: **2** if correct, **0** otherwise.\n",
        "\n",
        "Total reward per sample ‚àà {0, 1, 2, 3}.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLEkm7ec9HN"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 6: </b><br>\n",
        "write the `extract_xml_answer` function to extract the answer from the generated text.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjvdHWcVtUeX"
      },
      "outputs": [],
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = \"\"\n",
        "    for c in text:\n",
        "        if c.isdigit():\n",
        "            answer += c\n",
        "    return answer\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
        "    pattern = r\"^(?:[^\\r\\n]*\\r?\\n)+<answer>\\d+</answer>\\r?\\n?$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [bool(re.match(pattern, r)) for r in responses]\n",
        "    return [1.0 if match else 0.0 for match in matches]\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWWcLFhtUeX"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcuDydttUeY"
      },
      "outputs": [],
      "source": [
        "grpo_config = GRPOConfig(\n",
        "    output_dir=\"./grpo_model\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=3,\n",
        "    max_prompt_length=MAX_PROMPT_LEN,\n",
        "    max_completion_length=MAX_LENGTH,\n",
        "    num_generations=2, # The effective train batch size must be evenly divisible by the number of generations per prompt\n",
        "    beta=0,\n",
        "    epsilon=0.28,\n",
        "    temperature=1,\n",
        "    logging_steps=1,\n",
        "    save_steps=25,\n",
        "    save_total_limit=3,\n",
        "    # load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"reward\",\n",
        "    # greater_is_better=True,\n",
        "    run_name=RUN_NAME,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\" ,  # More memory efficient\n",
        "    warmup_ratio=0.03, # 3% of the steps will be just a warmup\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[format_reward_func, correctness_reward_func],\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting GRPO training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt3VtLn0tUeY"
      },
      "outputs": [],
      "source": [
        "save_path = \"grpo_model/final_merged_grpo_model\"\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgS802UtUeY"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ViMeNJ9tUeY"
      },
      "outputs": [],
      "source": [
        "from RLHF.chat_app import launch_chat_app\n",
        "\n",
        "launch_chat_app(\n",
        "    model_path=\"habdine/CSC_53432_lab3_grpo\",\n",
        "    base_model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    title=\"ü§ñ Dual-Model Qwen Chat (GRPO vs Base) for Math\",\n",
        "    DPO_TEST=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIpnugPkeStI"
      },
      "source": [
        "## üìà Model Evaluation\n",
        "\n",
        "Once our GRPO-trained model is ready, we need to **evaluate its performance**  ‚Äî to verify that it has learned to produce correctly formatted and accurate answers.\n",
        "\n",
        "For computational issues, we will evaluate on the first 200 samples only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEE-nS0I33TW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    try:\n",
        "        return text.split(\"####\")[1].strip()\n",
        "    except IndexError:\n",
        "        return None\n",
        "\n",
        "def evaluate_model(\n",
        "    model_path: str,\n",
        "    batch_size: int = 1,\n",
        "    num_samples: int = None,\n",
        "    save_results: bool = True,\n",
        ") -> Dict:\n",
        "    print(\"Initializing evaluation...\")\n",
        "\n",
        "    with tqdm(total=2, desc=\"Loading model components\") as pbar:\n",
        "        llm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"cuda:0\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_path,\n",
        "            model_max_length=768,\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "    # Load test dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
        "    if num_samples:\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "    total_samples = len(dataset)\n",
        "    print(f\"Loaded {total_samples} samples\")\n",
        "\n",
        "    results = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create progress bar\n",
        "    progress_bar = tqdm(\n",
        "        total=total_samples,\n",
        "        desc=\"Processing samples\",\n",
        "        unit=\"examples\",\n",
        "        dynamic_ncols=True,\n",
        "    )\n",
        "\n",
        "    progress_bar.set_postfix({\n",
        "        'acc': '0.00%',\n",
        "        'correct': '0',\n",
        "    })\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "        batch_data = dataset[i:i + batch_size]\n",
        "        current_batch_size = len(batch_data['question'])\n",
        "\n",
        "        # Prepare prompts using same format as training\n",
        "        prompts = [\n",
        "            [\n",
        "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
        "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
        "                {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n<answer>4</answer>\"},\n",
        "                {'role': 'user', 'content': q.strip()}\n",
        "            ] for q in batch_data['question']\n",
        "        ]\n",
        "\n",
        "        # Convert to chat format\n",
        "        formatted_prompts = [\n",
        "            tokenizer.apply_chat_template(\n",
        "                p,\n",
        "                tokenize=True,\n",
        "                return_tensors='pt',\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            for p in prompts\n",
        "        ]\n",
        "\n",
        "        # Generate responses\n",
        "        outputs = []\n",
        "        for prompt in formatted_prompts:\n",
        "            output = llm.generate(\n",
        "                prompt.to('cuda:0'),\n",
        "                max_new_tokens=512,\n",
        "                temperature=1.0,\n",
        "            )\n",
        "            outputs.append(output)\n",
        "\n",
        "\n",
        "        # Process responses\n",
        "        for j, output in enumerate(outputs):\n",
        "\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract answers\n",
        "            generated_answer = extract_xml_answer(response)\n",
        "            true_answer = extract_hash_answer(batch_data['answer'][j])\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                'question': batch_data['question'][j],\n",
        "                'true_answer': true_answer,\n",
        "                'generated_answer': generated_answer,\n",
        "                'full_response': response,\n",
        "                'correct': generated_answer == true_answer\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Update metrics\n",
        "            if generated_answer == true_answer:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        # Update progress\n",
        "        progress_bar.update(current_batch_size)\n",
        "        progress_bar.set_postfix({\n",
        "            'acc': f'{(correct/total)*100:.2f}%',\n",
        "            'correct': f'{correct}/{total}',\n",
        "        })\n",
        "\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'model_path': model_path,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    if save_results:\n",
        "        save_path = f\"gsm8k_eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'metrics': metrics,\n",
        "                'results': results\n",
        "            }, f, indent=2)\n",
        "        print(f\"\\nResults saved to {save_path}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"Starting GSM8K evaluation...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcED_4Npdms2"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 7: </b><br>\n",
        "evaluate the model before and after GRPO training.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqjBrw25ehbd"
      },
      "source": [
        "### Evaluation after GRPO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVBttE_aei4B"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating model AFTER GRPO\")\n",
        "metrics_after = evaluate_model(\n",
        "    model_path=\"habdine/CSC_53432_lab3_grpo\",\n",
        "    batch_size=1,\n",
        "    num_samples=200,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f'Results AFTER GRPO:')\n",
        "print(f'Accuracy: {metrics_after['accuracy']}')\n",
        "print(f'Correct: {metrics_after['correct']}/{metrics_after['total']}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfLz-PqQeoCO"
      },
      "source": [
        "### Evaluation before GRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4_cxUamWbKp"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating model AFTER GRPO\")\n",
        "metrics_before = evaluate_model(\n",
        "    model_path=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    batch_size=1,\n",
        "    num_samples=200,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f'Results AFTER GRPO:')\n",
        "print(f'Accuracy: {metrics_before['accuracy']}')\n",
        "print(f'Correct: {metrics_before['correct']}/{metrics_before['total']}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
